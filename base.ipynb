{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079baae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe2498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b49cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47200501",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = Path(data_dir)\n",
    "    \n",
    "    def load_tasks(self, split: str = \"training\") -> List[Dict[str, Any]]:\n",
    "        tasks = []\n",
    "        file_pattern = self.data_dir / split / \"*.json\"\n",
    "        \n",
    "        for file_path in glob.glob(str(file_pattern)):\n",
    "            with open(file_path, \"r\") as f:\n",
    "                task_data = json.load(f)\n",
    "                task = {\n",
    "                    \"task_id\": Path(file_path).stem,\n",
    "                    \"train\": task_data[\"train\"],\n",
    "                    \"test\": task_data[\"test\"]\n",
    "                }\n",
    "                tasks.append(task)\n",
    "        if not tasks:\n",
    "            raise FileNotFoundError(f\"No JSON files found in {file_pattern}\")\n",
    "        return tasks\n",
    "    \n",
    "    def augment_data(self, task: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        # Placeholder for data augmentation logic\n",
    "        # This could include transformations, noise addition, etc.\n",
    "        augmented_task = task.copy()\n",
    "        return augmented_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c61c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompter:\n",
    "    def __init__(self, template: str = None):\n",
    "        self.template = template or \"\"\"\n",
    "Task Description:\n",
    "Given input-output grid pairs, identify the transformation pattern and apply it to the test input.\n",
    "\n",
    "Training Examples:\n",
    "{train_examples}\n",
    "\n",
    "Test Input:\n",
    "{test_input}\n",
    "\n",
    "Please provide the output grid for the test input based on the pattern observed in the training examples.\n",
    "\"\"\"\n",
    "\n",
    "    def format_train_example(self, example: Dict[str, List[List[int]]]) -> str:\n",
    "        input_grid = np.array(example[\"input\"])\n",
    "        output_grid = np.array(example[\"output\"])\n",
    "        return f\"Input:\\n{input_grid.tolist()}\\nOutput:\\n{output_grid.tolist()}\\n\"\n",
    "\n",
    "    def create_prompt(self, task: Dict[str, Any]) -> str:\n",
    "        train_examples = \"\"\n",
    "        for example in task[\"train\"]:\n",
    "            train_examples += self.format_train_example(example) + \"\\n\"\n",
    "        \n",
    "        test_input = np.array(task[\"test\"][0][\"input\"]).tolist()\n",
    "        \n",
    "        return self.template.format(\n",
    "            train_examples=train_examples,\n",
    "            test_input=test_input\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, model: nn.Module):\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "    \n",
    "    def solve(self, prompt: str) -> List[List[int]]:\n",
    "        # Model inference logic would go here\n",
    "        # For now, we return a dummy output\n",
    "        return [[0, 0], [0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaadf0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        self.total_tasks = 0\n",
    "        self.correct_tasks = 0\n",
    "    \n",
    "    def evaluate(self, predicted: List[List[int]], ground_truth: List[List[int]]) -> bool:\n",
    "        predicted = np.array(predicted)\n",
    "        ground_truth = np.array(ground_truth)\n",
    "        return np.array_equal(predicted, ground_truth)\n",
    "    \n",
    "    def update_metrics(self, task: Dict[str, Any], prediction: List[List[int]]):\n",
    "        self.total_tasks += 1\n",
    "        ground_truth = task[\"test\"][0].get(\"output\")\n",
    "        if ground_truth and self.evaluate(prediction, ground_truth):\n",
    "            self.correct_tasks += 1\n",
    "            \n",
    "    def reset_metrics(self):\n",
    "        self.total_tasks = 0\n",
    "        self.correct_tasks = 0\n",
    "    \n",
    "    def get_accuracy(self) -> float:\n",
    "        return self.correct_tasks / self.total_tasks if self.total_tasks > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, data_loader: DataLoader, prompter: Prompter, solver: Solver, evaluator: Evaluator):\n",
    "        self.data_loader = data_loader\n",
    "        self.prompter = prompter\n",
    "        self.solver = solver\n",
    "        self.evaluator = evaluator\n",
    "        self.model = solver.model\n",
    "    \n",
    "    def train(self, split: str = \"training\", epochs: int = 1):\n",
    "        # TODO: Implement training logic\n",
    "        tasks = self.data_loader.load_tasks(split)\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "            for task in tasks:\n",
    "                augmented_task = self.data_loader.augment_data(task)\n",
    "                prompt = self.prompter.create_prompt(augmented_task)\n",
    "                prediction = self.solver.solve(prompt)\n",
    "                self.evaluator.update_metrics(task, prediction)\n",
    "                \n",
    "            accuracy = self.evaluator.get_accuracy()\n",
    "            print(f\"Accuracy after epoch {epoch + 1}: {accuracy:.2f}\")\n",
    "            self.evaluator.reset_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b4e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARC:\n",
    "    def __init__(self, data_loader: DataLoader, prompter: Prompter, solver: Solver, evaluator: Evaluator):\n",
    "        self.data_loader = data_loader\n",
    "        self.prompter = prompter\n",
    "        self.solver = solver\n",
    "        self.evaluator = evaluator\n",
    "        self.trainer = Trainer(data_loader, prompter, solver, evaluator)\n",
    "    \n",
    "    def arc_score(self):\n",
    "        self.trainer.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data_dir=\"data\")\n",
    "prompter = Prompter()\n",
    "solver = Solver(model)\n",
    "evaluator = Evaluator()\n",
    "arc = ARC(data_loader, prompter, solver, evaluator)\n",
    "arc.arc_score()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
